{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJVIBVRe6eDh"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KoichiFunaya/Colab/blob/dev/News_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvLkr5WUrxaG"
   },
   "source": [
    "# **Install Transformers Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lo_9LpxQrQGL",
    "outputId": "2dbed06c-f232-4970-9ab8-39b77d89719e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: requests in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: packaging in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\funaya\\anaconda3\\envs\\mvp21\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO8i-UxW6eDl"
   },
   "source": [
    "# **Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a8Up14aYrhB1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36anvHIM6eDm"
   },
   "source": [
    "## **Load Utility Class Instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GKnklyxB6eDn"
   },
   "outputs": [],
   "source": [
    "class NewsAnalyzer:\n",
    "    # annoate patterns\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # decide in which environment you are in\n",
    "        try:\n",
    "            # first try google.colab\n",
    "            from google.colab import drive\n",
    "            self.in_colab = True\n",
    "            print(\"in Google COLAB: load Google Drive and use GPU\")\n",
    "            self.hostname = \"GOOGLE_COLAB\"\n",
    "            # mount Google Drive\n",
    "            drive.mount('/content/drive')\n",
    "            # switch to the github folder\n",
    "            % cd /content/drive/MyDrive/Colab\\ Notebooks/colab\n",
    "            # pull the gitlab, to make sure it's up to date\n",
    "            ! git pull origin\n",
    "        except:\n",
    "            # we are not in google.colab\n",
    "            print(\"not in Google COLAB: access the data folder and use CPU\")\n",
    "            self.in_colab = False\n",
    "            # check what the hostname is\n",
    "            import socket\n",
    "            self.hostname=socket.gethostname()\n",
    "            print(\"hostname = {}\".format(self.hostname))\n",
    "\n",
    "        # load the configuration file \"config.ini\"\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read(\"config.ini\")\n",
    "        # set the path parameters\n",
    "        self.data_dir    = self.config[self.hostname]['data_dir']\n",
    "        self.sample_dir  = self.config[self.hostname]['sample_dir']\n",
    "        self.model_path  = self.config[self.hostname]['model_path']\n",
    "        self.label_path  = self.config[self.hostname]['label_path']\n",
    "        \n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def load_device(self):\n",
    "        \n",
    "        # load processor device\n",
    "        if self.in_colab:\n",
    "            # load CUDA\n",
    "            assert(torch.cuda.is_available())\n",
    "            self.has_cuda = True\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            try:\n",
    "                # check if CUDA device exists\n",
    "                assert(torch.cuda.is_available())\n",
    "                self.has_cuda = True\n",
    "            except:\n",
    "                # CUDA device does not exist\n",
    "                self.has_cuda = False\n",
    "            # in either case, we don't load CUDA, just use CPU.\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        return self.device, self.in_colab\n",
    "    \n",
    "    \n",
    "    def load_labelled_dataset(self):\n",
    "            \n",
    "        self.sent_df = pd.read_excel(self.label_path,sheet_name=self.config['LABEL_DATASET']['label_sheet'])\n",
    "            \n",
    "        return self.sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "7Tkb3SJS6eDo",
    "outputId": "844a81e3-e372-40ce-bdb5-83d40707e62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not in Google COLAB: access the data folder and use CPU\n",
      "hostname = N-1274\n"
     ]
    }
   ],
   "source": [
    "# create the utility class instance\n",
    "_NewsAnalyzer = NewsAnalyzer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDDdJjm2sECd"
   },
   "source": [
    "## **Load device and dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BGBlGaaW6eDp"
   },
   "outputs": [],
   "source": [
    "DEVICE,IN_COLAB = _NewsAnalyzer.load_device()\n",
    "DATA_DIR   = _NewsAnalyzer.data_dir\n",
    "MODEL_PATH = _NewsAnalyzer.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Mr-srUkkrq1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Date</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Object</th>\n",
       "      <th>Predicate</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time Annotated</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中国はこれまで長期にわたり、このような重要な新興分野で至急の導入が必要とされるハイレベルかつ...</td>\n",
       "      <td>2021-03-19 08:46:18.810</td>\n",
       "      <td>中国</td>\n",
       "      <td>苦境</td>\n",
       "      <td>立たす</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb</td>\n",
       "      <td>2021-03-21 21:07:48.290</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>キーテクノロジーを他国に握られることによる需給の矛盾が日増しに顕著になっている。</td>\n",
       "      <td>2021-03-19 08:46:18.810</td>\n",
       "      <td>他国</td>\n",
       "      <td>キーテクノロジーを</td>\n",
       "      <td>握る</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb</td>\n",
       "      <td>2021-03-21 21:09:06.837</td>\n",
       "      <td>受動態をあたかも能動態である可能ように扱っているので、間違い。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>このような背景の中、中国国産の「手で裂けるステンレス」の登場は、産業チェーンの不足を補い、効...</td>\n",
       "      <td>2021-03-19 08:46:18.810</td>\n",
       "      <td>中国</td>\n",
       "      <td>関連分野における中国の自主コントロール能力を</td>\n",
       "      <td>向上</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb</td>\n",
       "      <td>2021-03-21 21:10:19.541</td>\n",
       "      <td>「中国」という話題を記述できるようにすること。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n今日の中国は、世界一の製造大国としての地位がいっそう強固となった。</td>\n",
       "      <td>2021-03-19 08:46:18.810</td>\n",
       "      <td>中国</td>\n",
       "      <td>世界一の製造大国としての地位が</td>\n",
       "      <td>強固</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb</td>\n",
       "      <td>2021-03-21 21:11:38.487</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>こうしたデータの1つ1つは、中国が製造強国の建設を加速させる十分な底力と強い自信となった。</td>\n",
       "      <td>2021-03-19 08:46:18.810</td>\n",
       "      <td>中国</td>\n",
       "      <td>自信</td>\n",
       "      <td>なる</td>\n",
       "      <td>0</td>\n",
       "      <td>D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb</td>\n",
       "      <td>2021-03-21 21:12:49.857</td>\n",
       "      <td>主語である「中国」が存在しない</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence                    Date  \\\n",
       "0  中国はこれまで長期にわたり、このような重要な新興分野で至急の導入が必要とされるハイレベルかつ... 2021-03-19 08:46:18.810   \n",
       "1           キーテクノロジーを他国に握られることによる需給の矛盾が日増しに顕著になっている。 2021-03-19 08:46:18.810   \n",
       "2  このような背景の中、中国国産の「手で裂けるステンレス」の登場は、産業チェーンの不足を補い、効... 2021-03-19 08:46:18.810   \n",
       "3              \\n\\n今日の中国は、世界一の製造大国としての地位がいっそう強固となった。 2021-03-19 08:46:18.810   \n",
       "4      こうしたデータの1つ1つは、中国が製造強国の建設を加速させる十分な底力と強い自信となった。 2021-03-19 08:46:18.810   \n",
       "\n",
       "  Subject                  Object Predicate  Sentiment  \\\n",
       "0      中国                      苦境       立たす          0   \n",
       "1      他国               キーテクノロジーを        握る          0   \n",
       "2      中国  関連分野における中国の自主コントロール能力を        向上          0   \n",
       "3      中国         世界一の製造大国としての地位が        強固          0   \n",
       "4      中国                      自信        なる          0   \n",
       "\n",
       "                                     Source          Time Annotated  \\\n",
       "0  D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb 2021-03-21 21:07:48.290   \n",
       "1  D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb 2021-03-21 21:09:06.837   \n",
       "2  D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb 2021-03-21 21:10:19.541   \n",
       "3  D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb 2021-03-21 21:11:38.487   \n",
       "4  D:\\Personal\\Koichi\\DATA\\mvp21\\news\\afpbb 2021-03-21 21:12:49.857   \n",
       "\n",
       "                              Note  \n",
       "0                              NaN  \n",
       "1  受動態をあたかも能動態である可能ように扱っているので、間違い。  \n",
       "2          「中国」という話題を記述できるようにすること。  \n",
       "3                             ROOT  \n",
       "4                  主語である「中国」が存在しない  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = _NewsAnalyzer.load_labelled_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XGSY8AbFxkY4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_vGrT2mOxudI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    0.472050\n",
       "-1    0.440994\n",
       " 1    0.086957\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['Sentiment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfRnciXpxztF"
   },
   "source": [
    "## **Split train dataset into train, validation and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XkXatwzTxybE"
   },
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Sentence'], df['Sentiment'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['Sentiment'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XMZ7pSryAg4"
   },
   "source": [
    "## **Import BERT Model and BERT Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIgRg4q6x-Hg"
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgwlS0cU8fUb"
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cLCabhE8uTF"
   },
   "outputs": [],
   "source": [
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RUG-CnE9Vrj"
   },
   "source": [
    "# **Tokenize the Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svK4Ew5U9aqP"
   },
   "outputs": [],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y07SBCb3-Qmc"
   },
   "outputs": [],
   "source": [
    "max_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZX876Hg-Jek"
   },
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPdgs_Hb_LjT"
   },
   "source": [
    "## **Convert Integer Sequences to Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXlD2gm8-_-0"
   },
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpSc6NwH_K_7"
   },
   "source": [
    "## **Create DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTqJqfsO_UDp"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e160-CPw_uRs"
   },
   "source": [
    "## **Freeze BERT Parameters¶**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_YZP6ww_wZh"
   },
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA_4XlpI_3K0"
   },
   "source": [
    "## **Define Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nmbdFBE_7S9"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _outp  = self.bert(sent_id, attention_mask=mask)\n",
    "      cls_hs = _outp.pooler_output\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkmjX7gmAI9U"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5UMIGYCBWV0"
   },
   "outputs": [],
   "source": [
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNBjDm3SBn28"
   },
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVeh_opJE948"
   },
   "source": [
    "## **Find Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zN0SrSLB84j"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUrIWoq8D9Is"
   },
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4_EIIGZFdy0"
   },
   "source": [
    "## **Fine-Tune BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es3QzuZ5Fg0q"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQidFtqmFriZ"
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqW9zcQHFyF4"
   },
   "source": [
    "## **Start Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwASG802YFJw"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icU-d59kF0YP"
   },
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# measure time\n",
    "_start = _t0 = timer()\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "        # show the elapsed time\n",
    "    _t1 = timer()\n",
    "    print('Elapsed time: {}'.format(timedelta(seconds=_t1-_t0)))\n",
    "    _t0 = _t1\n",
    "    \n",
    "# show the total time for the training\n",
    "_t1 = timer()\n",
    "print('\\nTotal time elapsed: {}'.format(timedelta(seconds=_t1-_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJf8ngi3GjJm"
   },
   "source": [
    "## **Load Saved Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIMWET8EGkkn"
   },
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU46qL42GnjT"
   },
   "source": [
    "## **Get Predictions for Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aucsxsBiGrPD"
   },
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_72N4xBGtYb"
   },
   "outputs": [],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K70V_9OuGzpL"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5wwpG8Ltrr_"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    if input('OK to unmount drive? (y/n)')=='y':\n",
    "        drive.flush_and_unmount()\n",
    "        print('All changes made in this colab session should now be visible in Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "News_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
